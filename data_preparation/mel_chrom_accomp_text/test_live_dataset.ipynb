{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pypianoroll\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import io\n",
    "import symusic\n",
    "from pathlib import Path\n",
    "\n",
    "from chroma_subsystem.BinaryTokenizer import BinaryTokenizer, SimpleSerialChromaTokenizer\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, RobertaModel\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import midi_pianoroll_utils as mpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveMelCATDataset(Dataset):\n",
    "    def __init__(self, midis_folder, segment_size=64, resolution=24):\n",
    "        self.midis_folder = midis_folder\n",
    "        self.midis_list = os.listdir(midis_folder)\n",
    "        self.segment_size = segment_size\n",
    "        self.resolution = resolution\n",
    "        self.binary_chroma_tokenizer = SimpleSerialChromaTokenizer()\n",
    "        self.remi_tokenizer = REMI(params=Path('/media/datadisk/data/pretrained_models/midis_REMI_BPE_tokenizer.json'))\n",
    "        self.roberta_tokenizer_chroma = RobertaTokenizerFast.from_pretrained('/media/datadisk/data/pretrained_models/chroma_mlm_tiny/chroma_wordlevel_tokenizer')\n",
    "        self.roberta_tokenizer_midi = RobertaTokenizerFast.from_pretrained('/media/datadisk/data/pretrained_models/midi_mlm_tiny/midi_wordlevel_tokenizer')\n",
    "        self.roberta_tokenizer_text = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.padding_values = {\n",
    "            'melody': self.roberta_tokenizer_midi.pad_token_id,\n",
    "            'chroma': self.roberta_tokenizer_chroma.pad_token_id,\n",
    "            'text': self.roberta_tokenizer_text.pad_token_id,\n",
    "            'accomp': self.roberta_tokenizer_midi.pad_token_id\n",
    "        }\n",
    "        self.max_seq_lengths = {\n",
    "            'melody': 1024,\n",
    "            'chroma': 1024,\n",
    "            'text': 1024,\n",
    "            'accomp': 4096\n",
    "        }\n",
    "    # end init\n",
    "    def __len__(self):\n",
    "        return len(self.midis_list)\n",
    "    # end len\n",
    "    def __getitem__(self, idx):\n",
    "        print('idx:', idx)\n",
    "        print(self.midis_list[idx])\n",
    "        # load a midi file in pianoroll\n",
    "        main_piece = pypianoroll.read(self.midis_folder + os.sep + self.midis_list[idx], resolution=self.resolution)\n",
    "        main_piece_size = main_piece.downbeat.shape[0]\n",
    "        # make deepcopy\n",
    "        new_piece = deepcopy(main_piece)\n",
    "        # trim piece\n",
    "        start_idx = np.random.randint( main_piece_size - self.segment_size*main_piece.resolution )\n",
    "        end_idx = start_idx + self.segment_size*main_piece.resolution\n",
    "        new_piece.trim(start_idx, end_idx)\n",
    "        # split melody - accompaniment\n",
    "        melody_piece, accomp_piece = mpu.split_melody_accompaniment_from_pianoroll( new_piece )\n",
    "        # keep chroma from accompaniment\n",
    "        chroma_zoomed_out = mpu.chroma_from_pianoroll(accomp_piece)\n",
    "        # tokenize chroma to text tokens\n",
    "        tokenized_chroma = self.binary_chroma_tokenizer(chroma_zoomed_out)\n",
    "        chroma_string = ' '.join( tokenized_chroma['tokens'] )\n",
    "        chroma_tokens = self.roberta_tokenizer_chroma( chroma_string )\n",
    "        # make ghost files of melody and accomp pieces\n",
    "        melody_file = mpu.pianoroll_to_midi_bytes(melody_piece)\n",
    "        accomp_file = mpu.pianoroll_to_midi_bytes(accomp_piece)\n",
    "        # tokenize melody and accompaniment midi to text\n",
    "        remi_tokenized_melody = self.remi_tokenizer(melody_file)\n",
    "        melody_string = ' '.join(remi_tokenized_melody[0].tokens)\n",
    "        melody_tokens = self.roberta_tokenizer_midi(melody_string)\n",
    "        remi_tokenized_accomp = self.remi_tokenizer(accomp_file)\n",
    "        accomp_string = ' '.join(remi_tokenized_accomp[0].tokens)\n",
    "        accomp_tokens = self.roberta_tokenizer_midi(accomp_string)\n",
    "        # get text from title\n",
    "        text_description = self.midis_list[idx]\n",
    "        # tokenize text\n",
    "        text_tokens = self.roberta_tokenizer_text(text_description)\n",
    "        # return torch.LongTensor(melody_tokens['input_ids']),\n",
    "        return {\n",
    "            'melody': torch.LongTensor(melody_tokens['input_ids']),\n",
    "            'chroma': torch.LongTensor(chroma_tokens['input_ids']),\n",
    "            'text': torch.LongTensor(text_tokens['input_ids']),\n",
    "            'accomp': torch.LongTensor(accomp_tokens['input_ids'])\n",
    "        }\n",
    "    # end getitem\n",
    "\n",
    "    # def chroma_from_pianoroll(self, main_piece, resolution=24):\n",
    "    #     # first binarize a new deep copy\n",
    "    #     binary_piece = deepcopy(main_piece)\n",
    "    #     binary_piece.binarize()\n",
    "    #     # make chroma\n",
    "    #     chroma = binary_piece.tracks[0].pianoroll[:,:12]\n",
    "    #     for i in range(12, 128-12, 12):\n",
    "    #         chroma = np.logical_or(chroma, binary_piece.tracks[0].pianoroll[:,i:(i+12)])\n",
    "    #     chroma[:,-6:] = np.logical_or(chroma[:,-6:], binary_piece.tracks[0].pianoroll[:,-6:])\n",
    "    #     # quarter chroma resolution\n",
    "    #     chroma_tmp = np.zeros( (1,12) )\n",
    "    #     chroma_zoomed_out = None\n",
    "    #     for i in range(chroma.shape[0]):\n",
    "    #         chroma_tmp += chroma[i,:]\n",
    "    #         if (i+1)%resolution == 0:\n",
    "    #             if chroma_zoomed_out is None:\n",
    "    #                 chroma_zoomed_out = chroma_tmp >= np.mean( chroma_tmp )\n",
    "    #             else:\n",
    "    #                 chroma_zoomed_out = np.vstack( (chroma_zoomed_out, chroma_tmp >= np.mean( chroma_tmp )) )\n",
    "    #     if np.sum( chroma_tmp ) > 0:\n",
    "    #         if chroma_zoomed_out is None:\n",
    "    #             chroma_zoomed_out = chroma_tmp >= np.mean( chroma_tmp )\n",
    "    #         else:\n",
    "    #             chroma_zoomed_out = np.vstack( (chroma_zoomed_out, chroma_tmp >= np.mean( chroma_tmp )) )\n",
    "    #     return chroma_zoomed_out\n",
    "    # # end chroma_from_pianoroll\n",
    "\n",
    "    # def split_melody_accompaniment(self, pypianoroll_structure):\n",
    "    #     melody_piece = deepcopy( pypianoroll_structure )\n",
    "    #     accomp_piece = deepcopy( pypianoroll_structure )\n",
    "\n",
    "    #     mel_pr = melody_piece.tracks[0].pianoroll\n",
    "    #     acc_pr = accomp_piece.tracks[0].pianoroll\n",
    "\n",
    "    #     pr = np.array(melody_piece.tracks[0].pianoroll)\n",
    "    #     running_melody = -1\n",
    "    #     i = 0\n",
    "    #     # for i in range( pr.shape[0] ):\n",
    "    #     while i < pr.shape[0]:\n",
    "    #         # check if any note\n",
    "    #         if np.sum(pr[i,:]) > 0:\n",
    "    #             # get running max\n",
    "    #             running_max = np.max( np.nonzero( pr[i,:] ) )\n",
    "    #             # check if there exists a running melody\n",
    "    #             if running_melody > -1:\n",
    "    #                 # check if running melody is continued\n",
    "    #                 if running_melody == running_max:\n",
    "    #                     # remove all lower pitches from melody\n",
    "    #                     mel_pr[i, :running_max] = 0\n",
    "    #                     # remove higher pitch from accomp\n",
    "    #                     acc_pr[i, running_max] = 0\n",
    "    #                 else:\n",
    "    #                     # running melody may need to change\n",
    "    #                     # check if new highest pitch just started\n",
    "    #                     if running_max > running_melody:\n",
    "    #                         # a new higher note has started\n",
    "    #                         # finish previous note that was highest until now\n",
    "    #                         j = 0\n",
    "    #                         while j+i < mel_pr.shape[0] and mel_pr[i+j, running_melody] > 0 and running_max > running_melody:\n",
    "    #                             mel_pr[i+j, :running_melody] = 0\n",
    "    #                             mel_pr[i+j, running_melody+1:running_max] = 0\n",
    "    #                             acc_pr[i+j, running_melody] = 0\n",
    "    #                             acc_pr[i+j, running_max] = 0\n",
    "    #                             if np.sum( pr[i+j,:] ) > 0:\n",
    "    #                                 running_max = np.max( np.nonzero( pr[i+j,:] ) )\n",
    "    #                             else:\n",
    "    #                                 running_melody = -1\n",
    "    #                                 break\n",
    "    #                             j += 1\n",
    "    #                         # start new running melody\n",
    "    #                         i += j-1\n",
    "    #                         running_melody = running_max\n",
    "    #                     else:\n",
    "    #                         # i should be > 0 since we have that running_melody > -1\n",
    "    #                         # a lower note has come\n",
    "    #                         # if has begun earlier, it should be ignored\n",
    "    #                         if pr[i-1, running_max] > 0:\n",
    "    #                             # its continuing an existing note - not part of melody\n",
    "    #                             mel_pr[i, :] = 0\n",
    "    #                             # running max should not be canceled, it remains as ghost max\n",
    "    #                             # until a new higher max or a fresh lower max starts\n",
    "    #                         else:\n",
    "    #                             # a new fresh lower max starts that shouldn't be ignored\n",
    "    #                             # start new running melody\n",
    "    #                             running_melody = running_max\n",
    "    #                             # remove all lower pitches from melody\n",
    "    #                             mel_pr[i, :running_max] = 0\n",
    "    #                             # remove higher pitch from accomp\n",
    "    #                             acc_pr[i, running_max] = 0\n",
    "    #             else:\n",
    "    #                 # no running melody, check max conditions\n",
    "    #                 # new note started - make it the running melody\n",
    "    #                 running_melody = running_max\n",
    "    #                 # remove all lower pitches from melody\n",
    "    #                 mel_pr[i, :running_max] = 0\n",
    "    #                 # remove higher pitch from accomp\n",
    "    #                 acc_pr[i, running_max] = 0\n",
    "    #             # end if\n",
    "    #         else:\n",
    "    #             # there is a gap\n",
    "    #             running_melody = -1\n",
    "    #         # end if\n",
    "    #         i += 1\n",
    "    #     # end for\n",
    "    #     return melody_piece, accomp_piece\n",
    "    # # end split_melody_accompaniment\n",
    "\n",
    "    # def make_midi_bytes(self, pianoroll_structure):\n",
    "    #     # initialize bytes handle\n",
    "    #     b_handle = io.BytesIO()\n",
    "    #     # write midi data to bytes handle\n",
    "    #     pianoroll_structure.write(b_handle)\n",
    "    #     # start read pointer from the beginning\n",
    "    #     b_handle.seek(0)\n",
    "    #     # create a buffered reader to read the handle\n",
    "    #     buffered_reader = io.BufferedReader(b_handle)\n",
    "    #     # create a midi object from the \"file\", i.e., buffered reader\n",
    "    #     midi_bytes = symusic.Score.from_midi(b_handle.getvalue())\n",
    "    #     # close the bytes handle\n",
    "    #     b_handle.close()\n",
    "    #     return midi_bytes\n",
    "    # # end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "midifolder = '/media/datadisk/datasets/GiantMIDI-PIano/midis_v1.2/midis'\n",
    "# midifolder = '/media/datadisk/data/Giant_PIano/'\n",
    "dataset = LiveMelCATDataset(midifolder, segment_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10761\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\n",
      "Ismagilov, Timur, Spring Sketches, 2QxuHQoT5Dk.mid\n"
     ]
    }
   ],
   "source": [
    "d0 = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'melody': tensor([ 26,  70, 157,   5,  12,  52,  17,  20,   5,  12,   5,  10,  69,  59,\n",
       "          15,  11,   5,   8,   5,   9,  26,  90,  59,  24,   7,   5,  27,   5,\n",
       "           6,  66,  96,  21,   7,   5,  14,   5,   6,  82, 105,  21,  29,   5,\n",
       "           8,   5,  10,  26,  83,  52,  17,   7,   5,   8,   5,   6,  73,  52,\n",
       "         103,   7,   5,   9,   5,   6,  62,  38,  13,   7,   5,  14,   5,   6,\n",
       "          89,  44,  16,   7,   5,  14,   5,   6,  76,  52,  16,   7,   5,  22,\n",
       "           5,   6,  26,  73,  59,  19,  20,   5,   8,   5,  10,  26,  61,  44,\n",
       "          13,   7,   5,   9,   5,   6,  66,  52,  19,  11,   5,  12,   5,   9,\n",
       "          82,  96,  19,  11,   5,   8,   5,   9,  26,  79,  59,  13,  11,   5,\n",
       "          10,   5,   9,  26,  92,  59,  21,   7,   5,  25,   5,   6,  61,  96,\n",
       "          24,   7,   5,   9,   5,   6,  66, 105,  24,  20,   5,  12,   5,  10,\n",
       "          76, 105,  21,  95,   5,  12,   5,   8,  26,  26,  26,  67,  59,  13,\n",
       "          11,   5,   8,   5,   9,  82,  59,  21,   7,   5,  14,   5,   6,  83,\n",
       "          96,  24,   7,   5,  10,   5,   6,  88, 105,  24,  20,   5,  12,   5,\n",
       "          10,  72, 113,  36,  11,   5,  14,   5,   9]),\n",
       " 'chroma': tensor([25, 17,  9,  6,  7, 17,  9,  6,  7, 17,  9, 10,  6,  7, 17,  9, 10,  6,\n",
       "          7, 17,  9, 10,  6,  7, 17,  9, 10,  6,  7, 17,  9, 10,  6,  7, 17,  9,\n",
       "         10,  6,  7, 17,  9, 10,  6,  7, 17, 10,  6,  7, 17, 10,  6,  7, 17, 10,\n",
       "          6,  7, 17, 10,  6,  7, 17,  5, 10,  6,  7, 17,  5, 10,  6,  7, 17,  5,\n",
       "         10,  6,  7, 17,  5, 10,  6,  7, 17,  9,  5, 10,  6,  7, 17,  9,  5, 10,\n",
       "          6,  7, 17,  9,  5, 10,  6,  7, 17,  9,  5, 10,  6,  7, 17,  9,  5,  8,\n",
       "         10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17,\n",
       "          9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 11, 17,  9,  5,  8,\n",
       "         10,  6,  7, 11, 17,  9,  5,  8, 10,  6,  7, 11, 17,  9,  5,  8, 10,  6,\n",
       "          7, 11, 17,  9,  5,  8, 10,  6,  7, 11, 17,  9,  5,  8, 10,  6,  7, 11,\n",
       "         17,  9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17,  9,  5,  8,\n",
       "         10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17,\n",
       "          9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,\n",
       "          6,  7, 17,  9,  5,  8, 10,  6,  7, 17,  9,  5,  8, 10,  6,  7, 17]),\n",
       " 'text': tensor([    0,  6209, 16266,   718,  1417,     6,  2668,   710,     6,  5519,\n",
       "          4058,   594,  5559,     6,   132,  1864,  1178,   257, 37912,   139,\n",
       "           565,   245,   495,   330,     4, 16079,     2]),\n",
       " 'accomp': tensor([ 26,  70, 157,   5,  12,  35,  23,  11,   5,  14,   5,   9,  32,  18,\n",
       "          20,   5,  12,   5,  10,  31,  15,  11,   5,  10,   5,   9,  68,  37,\n",
       "          28,  29,   5,   8,   5,  10,  31,  17,  95,   5,  12,   5,   8,  26,\n",
       "          67,  52,  19,   7,   5,  10,   5,   6,  92,  44,  13,   7,   5,  14,\n",
       "           5,   6,  61,  52,  21,   7,   5,  14,   5,   6,  82,  32,  18,  29,\n",
       "           5,  12,   5,  10, 105,  21,   7,   5,   8,   5,   6,  96,  21,   7,\n",
       "           5,   8,   5,   6,  69,  37,  23,  29,   5,   8,   5,  10,  31,  13,\n",
       "          29,   5,  12,   5,  10,  26,  83,  32, 134,   7,   5,  22,   5,   6,\n",
       "          72,  44,  17,   7,   5,  14,   5,   6,  69,  31,  17,  20,   5,   8,\n",
       "           5,  10,  44,  15,  20,   5,   8,   5,  10,  26,  73,  31,  13,  20,\n",
       "           5,   8,   5,  10,  65,  33,  17,  20,   5,   8,   5,  10,  79,  52,\n",
       "          16,  11,   5,  10,   5,   9,  69,  44,  16,   7,   5,  22,   5,   6,\n",
       "          26,  67,  37,  15,  20,   5,   8,   5,  10,  38,  19,  20,   5,  12,\n",
       "           5,  10,  65,  59,  16,  20,   5,   8,   5,  10,  33,  18,  20,   5,\n",
       "           8,   5,  10,  76,  52,  13,  95,   5,  12,   5,   8,  26,  61,  35,\n",
       "          17, 114,   5,  12,   5,   8,  39,  13, 114,   5,  12,   5,   8,  80,\n",
       "          52,  19,   7,   5,  22,   5,   6,  26,  70,  44,  13,   7,   5,  22,\n",
       "           5,   6,  67,  52,  24,  11,   5,   8,   5,   9,  66,  31,  16,  20,\n",
       "           5,   8,   5,  10, 105,  24,   7,   5,   8,   5,   6,  65,  96,  21,\n",
       "          11,   5,   8,   5,   9,  76,  32,  15,  20,   5,   8,   5,  10,  33,\n",
       "          18, 124,   5,  12,   5,   8,  26,  68,  37,  28, 100,   5,  12,   5,\n",
       "           8,  96,  15, 100,   5,  12,   5,   8,  72,  59,  13,  29,   5,   8,\n",
       "           5,  10,  89,  52,  13,   7,   5,  22,   5,   6,  80,  32,  18,  20,\n",
       "           5,   8,   5,  10,  52,  16,  20,   5,   8,   5,  10,  26,  61,  44,\n",
       "          17,  95,   5,  12,   5,   8,  88,  31,  17, 100,   5,  12,   5,   8,\n",
       "          35,  23,  95,   5,  12,   5,   8,  26,  61,  52,  16,   7,   5,  14,\n",
       "           5,   6,  75,  44,  16,   7,   5,  14,   5,   6,  78,  52,  21,   7,\n",
       "           5,  14,   5,   6,  72,  32,  13,  11,   5,  14,   5,   9])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelCATCollator:\n",
    "    def __init__(self, max_seq_lens=None, padding_values=None):\n",
    "        self.max_seq_lens = max_seq_lens\n",
    "        self.padding_values = padding_values\n",
    "    # end init\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Create a dictionary to hold the batched data\n",
    "        batch_dict = {}\n",
    "        # Assume the batch is a list of dictionaries (each sample is a dict)\n",
    "        for key in batch[0]:\n",
    "            batch_dict[key] = {}\n",
    "            values = [item[key] for item in batch]\n",
    "            if isinstance(values[0], list):\n",
    "                # If values are lists (sequences of variable lengths), pad them\n",
    "                padded_values = pad_sequence([torch.tensor(v) for v in values], \n",
    "                                            batch_first=True, padding_value=self.padding_values[key])\n",
    "            elif isinstance(values[0], torch.Tensor):\n",
    "                # If values are tensors, stack them directly\n",
    "                padded_values = pad_sequence(values, batch_first=True, padding_value=self.padding_values[key])\n",
    "            # trim to max length\n",
    "            if self.max_seq_lens is not None and padded_values.shape[1] > self.max_seq_lens[key]:\n",
    "                padded_values = padded_values[:,:self.max_seq_lens[key]]\n",
    "            batch_dict[key]['input_ids'] = padded_values\n",
    "            attention_mask = torch.ones_like(padded_values, dtype=torch.long)\n",
    "            attention_mask[padded_values == self.padding_values[key]] = 0\n",
    "            batch_dict[key]['attention_mask'] = attention_mask\n",
    "        return batch_dict\n",
    "    # end call\n",
    "# end class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD\n",
    "# def custom_collate_fn(batch):\n",
    "#     # Create a dictionary to hold the batched data\n",
    "#     batch_dict = {}\n",
    "\n",
    "#     # Assume the batch is a list of dictionaries (each sample is a dict)\n",
    "#     for key in batch[0]:\n",
    "#         batch_dict[key] = {}\n",
    "#         values = [item[key] for item in batch]\n",
    "#         if isinstance(values[0], list):\n",
    "#             # If values are lists (sequences of variable lengths), pad them\n",
    "#             padded_values = pad_sequence([torch.tensor(v) for v in values], \n",
    "#                                          batch_first=True, padding_value=0)\n",
    "#             batch_dict[key]['input_ids'] = padded_values\n",
    "#             attention_mask = torch.ones_like(padded_values.shape, dtype=torch.long)\n",
    "#             attention_mask[padded_values == 0] = 0\n",
    "#             batch_dict[key]['attention_mask'] = attention_mask\n",
    "        \n",
    "#         elif isinstance(values[0], torch.Tensor):\n",
    "#             # If values are tensors, stack them directly\n",
    "#             batch_dict[key]['input_ids'] = pad_sequence(values, batch_first=True, padding_value=0)\n",
    "#             attention_mask = torch.ones_like(batch_dict[key]['input_ids'], dtype=torch.long)\n",
    "#             attention_mask[batch_dict[key]['input_ids'] == 0] = 0\n",
    "#             batch_dict[key]['attention_mask'] = attention_mask\n",
    "    \n",
    "#     return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_collate_fn = MelCATCollator(max_seq_lens=dataset.max_seq_lengths, padding_values=dataset.padding_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\n",
      "Ismagilov, Timur, Spring Sketches, 2QxuHQoT5Dk.mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1\n",
      "Gurlitt, Cornelius, Frühlingsblumen, Op.215, WD6wHfUb-kU.mid\n",
      "idx: 2\n",
      "Singelée, Jean Baptiste, Fantaisie sur des motifs de 'La sonnambula', Op.39, AcaSiJG7mkU.mid\n",
      "idx: 3\n",
      "Simpson, Daniel Léo, Kleine Klavierstücke No.9 in F major, R4z8vPF1Hto.mid\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'melody': {'input_ids': tensor([[ 26,  70, 157,  ...,   3,   3,   3],\n",
      "        [ 26,  70, 157,  ...,  12,   5,   9],\n",
      "        [ 26,  70, 157,  ...,   3,   3,   3],\n",
      "        [ 26,  70, 157,  ...,   3,   3,   3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, 'chroma': {'input_ids': tensor([[25, 17,  8,  ...,  7, 12, 17],\n",
      "        [25, 17,  9,  ...,  3,  3,  3],\n",
      "        [25, 17, 13,  ...,  3,  3,  3],\n",
      "        [25, 17,  5,  ...,  3,  3,  3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, 'text': {'input_ids': tensor([[    0,  6209, 16266,   718,  1417,     6,  2668,   710,     6,  5519,\n",
      "          4058,   594,  5559,     6,   132,  1864,  1178,   257, 37912,   139,\n",
      "           565,   245,   495,   330,     4, 16079,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   534,  6423,  2582,     6, 33053,  6125,     6,  4967,  2768,\n",
      "           298, 16283,  3662, 18546,     6,  7286,     4, 24355,     6, 33925,\n",
      "           401,   605,   725,   506, 44588,    12,   330,   791,     4, 16079,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 26292,   523,  9703,     6,  5363,  9457,   242,     6,   274,\n",
      "         11485,   354,   324,  8113,  2694, 32847,    29,   263,   128, 10766,\n",
      "           979,   282,  3146,  5571,  3934,  7286,     4,  3416,     6,  6208,\n",
      "           102, 35684,   863,   534,   406, 43503,   791,     4, 16079,     2],\n",
      "        [    0, 29656, 28898,     6,  3028,   226,  1140,   139,     6, 19361,\n",
      "           833,  7507, 37041,   620,  2768,   438,  1071,   440,     4,   466,\n",
      "            11,   274,   538,     6,   248,   306,   329,   398,   705, 16088,\n",
      "           134,   725,   560,     4, 16079,     2,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}, 'accomp': {'input_ids': tensor([[ 26,  70, 157,  ...,   3,   3,   3],\n",
      "        [ 26,  70, 157,  ...,   8,   5,   6],\n",
      "        [ 26,  70, 157,  ...,   3,   3,   3],\n",
      "        [ 26,  70, 157,  ...,   3,   3,   3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}}\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "# print(len(b['melody']['input_ids']))\n",
    "# print(len(b['accomp']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 402])\n",
      "torch.Size([4, 402])\n",
      "torch.Size([4, 40])\n",
      "torch.Size([4, 40])\n",
      "torch.Size([4, 301])\n",
      "torch.Size([4, 301])\n",
      "torch.Size([4, 889])\n",
      "torch.Size([4, 889])\n"
     ]
    }
   ],
   "source": [
    "print(b['melody']['input_ids'].shape)\n",
    "print(b['melody']['attention_mask'].shape)\n",
    "print(b['text']['input_ids'].shape)\n",
    "print(b['text']['attention_mask'].shape)\n",
    "print(b['chroma']['input_ids'].shape)\n",
    "print(b['chroma']['attention_mask'].shape)\n",
    "print(b['accomp']['input_ids'].shape)\n",
    "print(b['accomp']['attention_mask'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

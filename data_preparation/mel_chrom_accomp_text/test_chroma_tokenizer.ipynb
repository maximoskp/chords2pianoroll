{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Load your custom tokenizer from the JSON file\n",
    "tokenizer = Tokenizer.from_file('../data/chroma_wordlevel_tokenizer.json')\n",
    "\n",
    "# Load the trained tokenizer\n",
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_file='../data/chroma_tokenizer')\n",
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_file='../data/chroma_wordlevel_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<unk>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<s>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"</s>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<pad>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":4, \"content\":\"<mask>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Lowercase(), StripAccents()]), pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=WordLevel(vocab={\"<unk>\":0, \"<s>\":1, \"</s>\":2, \"<pad>\":3, \"<mask>\":4, ...}, unk_token=\"[UNK]\"))\n"
     ]
    }
   ],
   "source": [
    "# len(tokenizer.vocab.keys())\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained('../data/chroma_wordlevel_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/chroma_wordlevel_tokenizer/tokenizer_config.json',\n",
       " '../data/chroma_wordlevel_tokenizer/special_tokens_map.json',\n",
       " '../data/chroma_wordlevel_tokenizer/vocab.json',\n",
       " '../data/chroma_wordlevel_tokenizer/added_tokens.json',\n",
       " '../data/chroma_wordlevel_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap the tokenizer with RobertaTokenizerFast\n",
    "wrapped_tokenizer = RobertaTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# Add RoBERTa-specific special tokens\n",
    "# special_tokens = {\n",
    "#     \"unk_token\": \"<unk>\",\n",
    "#     \"cls_token\": \"<s>\",\n",
    "#     \"sep_token\": \"</s>\",\n",
    "#     \"pad_token\": \"<pad>\",\n",
    "#     \"mask_token\": \"<mask>\"\n",
    "# }\n",
    "# wrapped_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Save the tokenizer in a directory compatible with transformers\n",
    "wrapped_tokenizer.save_pretrained('../data/chroma_wordlevel_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizerFast(name_or_path='', vocab_size=26, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(wrapped_tokenizer.pad_token_id)\n",
    "print(len(wrapped_tokenizer.get_vocab()))\n",
    "print(wrapped_tokenizer.vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
